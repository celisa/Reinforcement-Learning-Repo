{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIPI 531 - HW 1 Q3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elisa Chen\n",
    "NetID: eyc11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stable_baselines3\n",
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "from stable_baselines3 import PPO, A2C #policy gradient\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from IPython import display as ipythondisplay"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a training and testing gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPERPARAMETERS\n",
    "game = \"CartPole-v1\"\n",
    "training_steps = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = gym.make(game)\n",
    "test_env = gym.make(game)\n",
    "test_env = Monitor(test_env, filename=None, allow_early_resets=True)\n",
    "\n",
    "#initializing a vanilla model\n",
    "vanilla_model = A2C('MlpPolicy', train_env, verbose = 0, tensorboard_log=\"./a2c_cartpole_tensorboard/\", gae_lambda = 0) \n",
    "\n",
    "#initializing a GAE advantage model\n",
    "gae_model = A2C('MlpPolicy', train_env, verbose = 0, tensorboard_log=\"./a2c_cartpole_tensorboard/\", gae_lambda = 0.85)\n",
    "\n",
    "#initializing n-step advantage model\n",
    "nstep_model = A2C('MlpPolicy', train_env, verbose = 0, tensorboard_log=\"./a2c_cartpole_tensorboard/\", gae_lambda = 1, n_steps = 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Rewards for Vanilla Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x7f0fd4675100>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Training the agent for 100000 steps\n",
    "vanilla_model.learn(total_timesteps=training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward: 42.05 +/- 15.45\n"
     ]
    }
   ],
   "source": [
    "## Evaluating the agent using 100 episodes\n",
    "reward_mean_vanilla, reward_std_vanilla = evaluate_policy(vanilla_model, test_env, n_eval_episodes=100)\n",
    "print(f'Mean Reward: {np.round(reward_mean_vanilla,2)} +/- {np.round(reward_std_vanilla,2)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Rewards for N-Step Advantage Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x7f0f011ec790>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the agent for 100000 steps\n",
    "nstep_model.learn(total_timesteps=training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward: 258.18 +/- 29.89\n"
     ]
    }
   ],
   "source": [
    "## Evaluating the agent using 100 episodes\n",
    "reward_mean_n_step, reward_std_n_step = evaluate_policy(nstep_model, test_env, n_eval_episodes=100)\n",
    "print(f'Mean Reward: {np.round(reward_mean_n_step,2)} +/- {np.round(reward_std_n_step,2)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Rewards for GAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x7f0efc8fa190>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the agent for 100000 steps\n",
    "gae_model.learn(total_timesteps=training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward: 251.43 +/- 27.91\n"
     ]
    }
   ],
   "source": [
    "## Evaluating the agent using 100 episodes\n",
    "reward_mean_gae, reward_std_gae = evaluate_policy(nstep_model, test_env, n_eval_episodes=100)\n",
    "print(f'Mean Reward: {np.round(reward_mean_gae,2)} +/- {np.round(reward_std_gae,2)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mean Reward For The Vanilla Model Is: 42.05 +/- 15.45\n",
      "The Mean Reward For The N-Step Advantage Model Is: 258.18 +/- 29.89\n",
      "The Mean Reward For The GAE Model Is: 251.43 +/- 27.91\n"
     ]
    }
   ],
   "source": [
    "print(f\"The Mean Reward For The Vanilla Model Is: {np.round(reward_mean_vanilla,2)} +/- {np.round(reward_std_vanilla,2)}\")\n",
    "print(f\"The Mean Reward For The N-Step Advantage Model Is: {np.round(reward_mean_n_step,2)} +/- {np.round(reward_std_n_step,2)}\")\n",
    "print(f\"The Mean Reward For The GAE Model Is: {np.round(reward_mean_gae,2)} +/- {np.round(reward_std_gae,2)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the N-Step Advantage Model fits the training data the best with a mean reward of 258.18. The GAE model produces similar results with a mean reward of 251.43. Both N-Step and GAE models have a large standard deviation of ~30 indicating that there might not be a significant difference between the performance of GAE and N-step Advantage models. We can observe that the Vanilla model produces the worst performance with a mean reward of ~42.05."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e8ba96d35f524bf788e0a066ae8555804e9c3c2c45d7eaf088b255e4838dd8cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
